apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: onion-analysis-pipeline-
  annotations:
    pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
    pipelines.kubeflow.org/pipeline_compilation_time: '2023-02-21T16:21:53.766060'
    pipelines.kubeflow.org/pipeline_spec: '{"inputs": [{"name": "from_day", "type":
      "Integer"}, {"name": "from_month", "type": "Integer"}, {"name": "from_year",
      "type": "Integer"}, {"default": "", "name": "pipeline-root"}, {"default": "pipeline/onion-analysis-pipeline",
      "name": "pipeline-name"}], "name": "onion-analysis-pipeline"}'
    pipelines.kubeflow.org/v2_pipeline: "true"
  labels:
    pipelines.kubeflow.org/v2_pipeline: "true"
    pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
spec:
  entrypoint: onion-analysis-pipeline
  templates:
  - name: classify-onions
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'nltk' 'plotly' 'pandas' 'bertopic==0.14.0' 'minio' 'stopwordsiso' 'kfp==1.8.18' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def classify_onions(from_day:int, from_month:int, from_year:int, language_output: Input[Artifact], metrics: Output[Metrics], plot: Output[HTML]):

            # Get the list of onions with first language is English
            import pickle
            with open(language_output.path, 'rb') as f:
                onion_languages = pickle.load(f)
            onion_list = [onion for onion in onion_languages if onion_languages[onion]['first_language'] == 'English']


            # Get the distribution of number of words and number of sentences in the onion_list to write in plot
            import nltk
            nltk.download('punkt')
            from nltk.tokenize import sent_tokenize, word_tokenize
            import pandas as pd
            import numpy as np
            import plotly.graph_objects as go
            import plotly.io as pio

            onion_sentences = []
            onion_words = []
            for onion in onion_list:
                onion_sentences.append(len(sent_tokenize(onion)))
                onion_words.append(len(word_tokenize(onion)))

            df = pd.DataFrame({'onion_sentences': onion_sentences, 'onion_words': onion_words})
            df2 = df.groupby('onion_sentences').count().reset_index()
            df2.columns = ['onion_sentences', 'count']
            #fig = go.Figure(data=[go.Bar(x=df.onion_sentences, y=df.count)])
            fig = go.Figure(data=[go.Box(y=df2.onion_sentences, boxpoints='all', jitter=0.3, pointpos=-1.8)])
            fig.update_layout(title=f'Number of sentences per onion until {from_day}/{from_month}/{from_year}')

            df3 = df.groupby('onion_words').count().reset_index()
            df3.columns = ['onion_words', 'count']
            fig2 = go.Figure(data=[go.Box(y=df3.onion_words, boxpoints='all', jitter=0.3, pointpos=-1.8)])
            fig2.update_layout(title=f'Number of words per onion until {from_day}/{from_month}/{from_year}')

            with open(plot.path, 'w') as p:
                p.write(pio.to_html(fig)+pio.to_html(fig2))

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - classify_onions
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, classify-onions, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'from_day={{inputs.parameters.from_day}}',
        'from_month={{inputs.parameters.from_month}}', 'from_year={{inputs.parameters.from_year}}',
        --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.8-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"from_day": {"type":
          "INT"}, "from_month": {"type": "INT"}, "from_year": {"type": "INT"}}, "inputArtifacts":
          {"language_output": {"metadataPath": "/tmp/inputs/language_output/data",
          "schemaTitle": "system.Artifact", "instanceSchema": "", "schemaVersion":
          "0.0.1"}}, "outputParameters": {}, "outputArtifacts": {"metrics": {"schemaTitle":
          "system.Metrics", "instanceSchema": "", "schemaVersion": "0.0.1", "metadataPath":
          "/tmp/outputs/metrics/data"}, "plot": {"schemaTitle": "system.HTML", "instanceSchema":
          "", "schemaVersion": "0.0.1", "metadataPath": "/tmp/outputs/plot/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.8-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: from_day}
      - {name: from_month}
      - {name: from_year}
      - {name: pipeline-name}
      - {name: pipeline-root}
      artifacts:
      - {name: identify-languages-language_output, path: /tmp/inputs/language_output/data}
    outputs:
      artifacts:
      - {name: classify-onions-metrics, path: /tmp/outputs/metrics/data}
      - {name: classify-onions-plot, path: /tmp/outputs/plot/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
        pipelines.kubeflow.org/arguments.parameters: '{"from_day": "{{inputs.parameters.from_day}}",
          "from_month": "{{inputs.parameters.from_month}}", "from_year": "{{inputs.parameters.from_year}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: deduplicate-onions
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'numpy' 'minio' 'plotly' 'datasketch' 'kfp==1.8.18' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
        \ import *\n\ndef deduplicate_onions(from_day:int, from_month:int, from_year:int,\
        \ preprocessed_onions: Input[Artifact], exactduplicated_onions: Output[Artifact],\
        \ nearduplicated_onions: Output[Artifact], metrics: Output[Metrics], plot:\
        \ Output[HTML]):\n    from minio import Minio\n    import pickle\n    from\
        \ datetime import date, timedelta\n    from minio.error import S3Error\n \
        \   import io\n    import plotly.graph_objs as go\n    import plotly.io as\
        \ pio\n\n    client = Minio(\n        endpoint=\"minio.minio:9000\",\n   \
        \     secure=False,\n        access_key='processing',\n        secret_key=',TkTV3c7:e6#e}HwiD4R'\n\
        \    )\n\n    with open(preprocessed_onions.path, 'rb') as f:\n        new_onions\
        \ = pickle.load(f)\n\n\n    # download last version\n    first_version = date(2022,\
        \ 11, 15)          # threshold of first version\n    last_date = date(from_year,\
        \ from_month, from_day)                     # dates to check from now to origin\n\
        \    exact_onions = None                    # indicates if previous version\
        \ found\n    while (last_date > first_version and exact_onions is None):\n\
        \        last_date = last_date - timedelta(days=1)\n        print(\"Checking\
        \ version of\", last_date.strftime(\"%d-%m-%Y\"), \"...\")\n        try:\n\
        \            response = client.get_object(\"datasets\",f\"{last_date.year}/{last_date.month}/{last_date.day}/deduplicated_onions/exact_onions.pickle\"\
        )\n            exact_onions = pickle.loads(response.read())\n        except\
        \ S3Error as exc:\n            print(\"It does not exist! Next...\", exc)\n\
        \            continue\n\n    # calculate exact duplicates\n    if exact_onions\
        \ is None:\n        exact_onions = {}\n\n    total_onions = 0\n    exact_duplicates\
        \ = 0\n    non_exact_duplicates = 0\n    new_unique_onions = []\n    for address,text\
        \ in new_onions.items():\n        if text != '':\n            total_onions\
        \ += 1\n            result = exact_onions.get(text, False)\n            if\
        \ result is False:\n                non_exact_duplicates += 1\n          \
        \      exact_onions[text] = [address]\n                new_unique_onions.append(text)\n\
        \            else:\n                exact_onions[text].append(address)\n \
        \               exact_duplicates += 1\n\n    with open(exactduplicated_onions.path,\
        \ 'wb') as f:\n        pickle.dump(exact_onions, f)\n\n    # calculate near\
        \ duplicates\n\n    from collections import defaultdict\n    from datasketch\
        \ import MinHash, MinHashLSH\n\n    def union_of_intersecting_arrays(arr1,\
        \ arr2):\n        # Create a set from each array\n        s1 = set(arr1)\n\
        \        s2 = set(arr2)\n\n        # Check if the sets intersect\n       \
        \ if s1.intersection(s2):\n            # Return the union of the sets as a\
        \ list\n            return list(s1.union(s2))\n        else:\n           \
        \ # If the sets do not intersect, return False\n            return []\n\n\
        \    def group_duplicate_texts_minhashlsh(texts):\n\n        # Create a MinHash\
        \ LSH index\n        lsh = MinHashLSH(threshold=0.5, num_perm=128)\n\n   \
        \     # For each text in the list of texts, create a MinHash object and add\
        \ it to the index\n        for i,text in enumerate(texts):\n            minhash\
        \ = MinHash(num_perm=128)\n            for word in text.split():\n       \
        \         minhash.update(word.encode('utf8'))\n            lsh.insert(text,\
        \ minhash)\n\n        # For each text in the list of texts, find the groups\
        \ of duplicates\n        duplicate_groups = {}\n        nduplicates=0\n  \
        \      for text in texts:\n            minhash = MinHash(num_perm=128)\n \
        \           for word in text.split():\n                minhash.update(word.encode('utf8'))\n\
        \            duplicates = lsh.query(minhash)\n\n            found = False\n\
        \            for duplicate_address, duplicate_group in duplicate_groups.items():\n\
        \                union = union_of_intersecting_arrays(duplicates, duplicate_group)\
        \     # go through all the array, not only the first\n                if union!=[]:\n\
        \                    found = True\n                    duplicate_groups[duplicate_address]\
        \ = union\n                    break\n\n            if not found:\n      \
        \          duplicate_groups[text] = duplicates\n\n        return duplicate_groups\n\
        \n\n    print(f\"Deduplicating {len(exact_onions)} onions...\")\n    duplicates_minhash\
        \ = group_duplicate_texts_minhashlsh(list(exact_onions.keys()))\n    duplicates_minhash_2\
        \ = {}\n    duplicated_onions = []\n    for onion, duplicates in duplicates_minhash.items():\n\
        \n        for duplicate in duplicates:\n            if duplicate not in duplicated_onions:\
        \ \n                if duplicates_minhash_2.get(onion, False) is False:\n\
        \                    duplicates_minhash_2[onion] = []\n                duplicates_minhash_2[onion].append(duplicate)\n\
        \                duplicated_onions.append(duplicate)\n\n    duplicates_minhash\
        \ = duplicates_minhash_2\n\n    ## Count near duplicates and get the longer\
        \ onion service as the representative one.\n    final_unique_onions = 0\n\
        \    near_duplic_onions = 0\n    near_duplicates = {}\n\n\n    for onion,\
        \ near_duplicated_onions in duplicates_minhash.items():\n\n        # get the\
        \ representative onion\n        representative_onion = onion\n        for\
        \ near_duplicated_onion in near_duplicated_onions:\n            if len(near_duplicated_onion)\
        \ > len(representative_onion):\n                representative_onion = near_duplicated_onion\n\
        \        near_duplicates[representative_onion] = near_duplicated_onions\n\n\
        \        # check new unique and near-duplic onions\n        for ndo in near_duplicates[representative_onion]:\n\
        \            if ndo != representative_onion and ndo in new_unique_onions:\n\
        \                near_duplic_onions += 1\n            elif ndo == representative_onion\
        \ and ndo in new_unique_onions:\n                final_unique_onions += 1\n\
        \n    with open(nearduplicated_onions.path, 'wb') as f:\n        pickle.dump(near_duplicates,\
        \ f)\n\n\n    #### metrics and plot\n    yesterday = date.today() - timedelta(days=1)\n\
        \n    fig = go.Figure(data=[go.Pie(labels=['Exact mirrors', 'Near mirrors',\
        \ 'Unique onions'],\n                                 values=[exact_duplicates,\
        \ near_duplic_onions, final_unique_onions])])\n    fig.update_layout(title=f'Deduplicated\
        \ onions from {from_day}/{from_month}/{from_year} to {yesterday.day}/{yesterday.month}/{yesterday.year}')\n\
        \    with open(plot.path, 'w') as p:\n        p.write(pio.to_html(fig))\n\n\
        \    metrics.log_metric('Total onions', total_onions)\n    metrics.log_metric('Exact\
        \ mirrors', exact_duplicates)\n    metrics.log_metric('Near mirrors', near_duplic_onions)\n\
        \    metrics.log_metric('Unique onions', final_unique_onions)\n\n"
      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - deduplicate_onions
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, deduplicate-onions, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'from_day={{inputs.parameters.from_day}}',
        'from_month={{inputs.parameters.from_month}}', 'from_year={{inputs.parameters.from_year}}',
        --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.8-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"from_day": {"type":
          "INT"}, "from_month": {"type": "INT"}, "from_year": {"type": "INT"}}, "inputArtifacts":
          {"preprocessed_onions": {"metadataPath": "/tmp/inputs/preprocessed_onions/data",
          "schemaTitle": "system.Artifact", "instanceSchema": "", "schemaVersion":
          "0.0.1"}}, "outputParameters": {}, "outputArtifacts": {"exactduplicated_onions":
          {"schemaTitle": "system.Artifact", "instanceSchema": "", "schemaVersion":
          "0.0.1", "metadataPath": "/tmp/outputs/exactduplicated_onions/data"}, "metrics":
          {"schemaTitle": "system.Metrics", "instanceSchema": "", "schemaVersion":
          "0.0.1", "metadataPath": "/tmp/outputs/metrics/data"}, "nearduplicated_onions":
          {"schemaTitle": "system.Artifact", "instanceSchema": "", "schemaVersion":
          "0.0.1", "metadataPath": "/tmp/outputs/nearduplicated_onions/data"}, "plot":
          {"schemaTitle": "system.HTML", "instanceSchema": "", "schemaVersion": "0.0.1",
          "metadataPath": "/tmp/outputs/plot/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.8-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: from_day}
      - {name: from_month}
      - {name: from_year}
      - {name: pipeline-name}
      - {name: pipeline-root}
      artifacts:
      - {name: preprocess-onions-preprocessed_onions, path: /tmp/inputs/preprocessed_onions/data}
    outputs:
      artifacts:
      - {name: deduplicate-onions-exactduplicated_onions, path: /tmp/outputs/exactduplicated_onions/data}
      - {name: deduplicate-onions-metrics, path: /tmp/outputs/metrics/data}
      - {name: deduplicate-onions-nearduplicated_onions, path: /tmp/outputs/nearduplicated_onions/data}
      - {name: deduplicate-onions-plot, path: /tmp/outputs/plot/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
        pipelines.kubeflow.org/arguments.parameters: '{"from_day": "{{inputs.parameters.from_day}}",
          "from_month": "{{inputs.parameters.from_month}}", "from_year": "{{inputs.parameters.from_year}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: download-onions
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'minio' 'bs4' 'trafilatura' 'plotly' 'kfp==1.8.18' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def download_onions(from_day:int, from_month:int, from_year:int, cleaned_onions: Output[Artifact], metrics: Output[Metrics], plot: Output[HTML]):
            from minio import Minio
            from datetime import date, timedelta
            from bs4 import BeautifulSoup
            from trafilatura import extract
            import string
            import pickle
            import plotly.graph_objs as go
            import plotly.io as pio

            def daterange(start_date, end_date):
                for n in range(int((end_date - start_date).days)):
                    yield start_date + timedelta(n)

            def get_title(soup):
                if soup.findAll("title"):
                    return soup.find("title").string
                else:
                    return ''

            def get_description(soup):
                if soup.findAll("meta", attrs={"name": "description"}):
                    return soup.find("meta", attrs={"name": "description"}).get("content")
                else:
                    return ''

            client = Minio(
                endpoint="minio.minio:9000",
                secure=False,
                access_key='processing',
                secret_key=',TkTV3c7:e6#e}HwiD4R'
            )

            start_date = date(from_year, from_month, from_day)
            today = date.today()

            total_onions = 0
            empty_onions = 0
            bad_encoded_onions = 0
            debug = 0

            htmls = {}
            for single_date in daterange(start_date, today):
                daily_onions = 0
                if single_date < today:
                    print("Downloading data from",single_date.strftime("%d-%m-%Y"), "...")
                    for document in client.list_objects(bucket_name="onions", prefix=f"{single_date.year}/{single_date.month}/{single_date.day}/htmls/"):
                        total_onions+=1
                        daily_onions+=1
                        try:
                            response = client.get_object("onions", document.object_name)
                            html = response.data.decode("utf-8")
                            bs = BeautifulSoup(html, 'html.parser')

                            title = get_title(bs)
                            if title is None:
                                title = ''
                            description = get_description(bs)
                            if description is None:
                                description = ''
                            content = ''
                            content = extract(html, include_formatting=False, include_links=False, include_images=False, include_tables=False)
                            if content is None:
                                content = ''
                            title_description_content = []
                            if title == description:
                                title_description_content = [title, content]
                            else:
                                title_description_content = [title, description, content]
                            text = ''
                            for t in title_description_content:
                                if t != '':
                                    if t[:-1] != text:
                                        if t[-1] not in string.punctuation:
                                            t = t+'. '
                                        else:
                                            t = t+' '
                                        text = "{0}{1}".format(text, t)

                            if text == '':
                                empty_onions+=1
                            #print(f"[Title] {title} [Description] {description}")
                            htmls[document.object_name] = text

                            if total_onions % 100 == 0:
                                print(f"Processed {total_onions} onions...")
                                #break

                        except UnicodeDecodeError as err:
                            bad_encoded_onions+=1
                            print(f"[Error] {err}")
                            htmls[document.object_name] = 'bad encoded'
                            continue
                        except Exception as err:
                            bad_encoded_onions+=1
                            print(f"[Error] {err}")
                            htmls[document.object_name] = 'bad encoded'
                            continue
                        finally:
                            response.close()
                            response.release_conn()

                    print("Downloaded onions in",single_date.strftime("%d-%m-%Y"),"->",daily_onions)


            yesterday = today - timedelta(days=1)
            fig = go.Figure(data=[go.Pie(labels=['Cleaned onions', 'Empty onions', 'Bad-encoded onions'],
                                         values=[total_onions-empty_onions-bad_encoded_onions, empty_onions, bad_encoded_onions])])
            fig.update_layout(title=f'Downloaded onions from {from_day}/{from_month}/{from_year} to {yesterday.day}/{yesterday.month}/{yesterday.year}')

            with open(plot.path, 'w') as p:
                p.write(pio.to_html(fig))

            metrics.log_metric('Total onions', total_onions)
            metrics.log_metric('Empty onions', empty_onions)
            metrics.log_metric('Bad-encoded onions', bad_encoded_onions)
            metrics.log_metric('Cleaned onions', total_onions-empty_onions-bad_encoded_onions)

            with open(cleaned_onions.path, 'wb') as f:
                pickle.dump(htmls, f)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - download_onions
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, download-onions, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'from_day={{inputs.parameters.from_day}}',
        'from_month={{inputs.parameters.from_month}}', 'from_year={{inputs.parameters.from_year}}',
        --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.9-alpine'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"from_day": {"type":
          "INT"}, "from_month": {"type": "INT"}, "from_year": {"type": "INT"}}, "inputArtifacts":
          {}, "outputParameters": {}, "outputArtifacts": {"cleaned_onions": {"schemaTitle":
          "system.Artifact", "instanceSchema": "", "schemaVersion": "0.0.1", "metadataPath":
          "/tmp/outputs/cleaned_onions/data"}, "metrics": {"schemaTitle": "system.Metrics",
          "instanceSchema": "", "schemaVersion": "0.0.1", "metadataPath": "/tmp/outputs/metrics/data"},
          "plot": {"schemaTitle": "system.HTML", "instanceSchema": "", "schemaVersion":
          "0.0.1", "metadataPath": "/tmp/outputs/plot/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.9-alpine
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: from_day}
      - {name: from_month}
      - {name: from_year}
      - {name: pipeline-name}
      - {name: pipeline-root}
    outputs:
      artifacts:
      - {name: download-onions-cleaned_onions, path: /tmp/outputs/cleaned_onions/data}
      - {name: download-onions-metrics, path: /tmp/outputs/metrics/data}
      - {name: download-onions-plot, path: /tmp/outputs/plot/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
        pipelines.kubeflow.org/arguments.parameters: '{"from_day": "{{inputs.parameters.from_day}}",
          "from_month": "{{inputs.parameters.from_month}}", "from_year": "{{inputs.parameters.from_year}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: identify-languages
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'minio' 'fasttext' 'pycountry' 'pandas' 'plotly' 'kfp==1.8.18' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
        \ import *\n\ndef identify_languages(unique_onions: Input[Artifact], language_output:\
        \ Output[Artifact], plot: Output[HTML], metrics: Output[Metrics]):\n    import\
        \ fasttext\n    from pycountry import languages\n    from datetime import\
        \ date, timedelta\n    import pickle\n\n    class LanguageIdentification:\n\
        \n        def __init__(self):\n            # connect to MinIO and download\
        \ the model\n            from minio import Minio\n            import io\n\
        \            client = Minio(\n                endpoint=\"minio.minio:9000\"\
        ,\n                secure=False,\n                access_key='processing',\n\
        \                secret_key=',TkTV3c7:e6#e}HwiD4R')\n\n            try:\n\
        \                client.fget_object(bucket_name='models', object_name='language/lid.176.bin',\
        \ file_path='/tmp/lid.176.bin')\n            except:\n                print(\"\
        Error downloading the model\")\n\n            pretrained_lang_model = '/tmp/lid.176.bin'\n\
        \            self.model = fasttext.load_model(pretrained_lang_model)\n\n \
        \       def predict_lang(self, text):\n            predictions = self.model.predict(text,\
        \ k=2) # returns top 2 matching languages\n            return predictions\n\
        \n    def get_languages(text):\n\n        LANGUAGE = LanguageIdentification()\n\
        \n        lang = LANGUAGE.predict_lang(text)\n\n        ### language distribution\n\
        \        first_language, second_language = None, None\n        try:\n    \
        \        first_language = languages.get(alpha_2=lang[0][0].split(\"__label__\"\
        )[1]).name\n        except:\n            first_language = lang[0][0].split(\"\
        __label__\")[1]\n\n        first_confidence = lang[1][0]\n\n        try:\n\
        \            second_language = languages.get(alpha_2=lang[0][1].split(\"__label__\"\
        )[1]).name\n        except:\n            second_language = lang[0][1].split(\"\
        __label__\")[1]\n\n        second_confidence = lang[1][1]\n\n        return\
        \ first_language, first_confidence, second_language, second_confidence\n\n\
        \n    # read pickle file with unique onions\n    with open(unique_onions.path,\
        \ 'rb') as f:\n        onions = pickle.load(f)\n\n    dict_lang = {'first_language':\
        \ [],\n                'first_confidence': [],\n                'second_language':\
        \ [],\n                'second_confidence': []}\n\n    onion_languages = {}\n\
        \n\n    for onion in list(onions.keys()):\n        first_language, first_confidence,\
        \ second_language, second_confidence = get_languages(onion)\n        dict_lang['first_language'].append(first_language)\
        \  \n        dict_lang['first_confidence'].append(first_confidence)\n    \
        \    dict_lang['second_language'].append(second_language)  \n        dict_lang['second_confidence'].append(second_confidence)\n\
        \n        onion_languages[onion] = {}\n        onion_languages[onion]['first_language']\
        \ = first_language \n        onion_languages[onion]['first_confidence'] =\
        \ first_confidence\n        onion_languages[onion]['second_language'] = second_language\n\
        \        onion_languages[onion]['second_confidence'] = second_confidence\n\
        \n\n    # write a pie chart in plot argument with the distribution of first\
        \ language of the onions which is in 'first_language' list of dict_lang\n\
        \    import pandas as pd\n    import plotly.graph_objects as go\n    import\
        \ plotly.io as pio\n\n    df = pd.DataFrame(dict_lang)\n    df = df.first_language.value_counts()/len(df)*100\n\
        \    df = df.reset_index()\n    df.columns = ['language', 'percentage']\n\
        \    df = df.sort_values(by='percentage', ascending=False)\n    df = df.head(10)\n\
        \    fig = go.Figure(data=[go.Pie(labels=df.language, values=df.percentage)])\n\
        \n    versioning_date = date.today() - timedelta(days=1)\n    fig.update_layout(title=f'Language\
        \ distribution of onions until {versioning_date.day}/{versioning_date.month}/{versioning_date.year}')\n\
        \    with open(plot.path, 'w') as p:\n        p.write(pio.to_html(fig))\n\n\
        \n    # write the onion_languages dictionary in the language_output artifact\
        \ as a pickle\n    with open(language_output.path, 'wb') as f:\n        pickle.dump(onion_languages,\
        \ f)\n\n    # write in metrics the number of onions per language\n    metrics.log_metric('Total\
        \ onions', len(onion_languages))\n    for language in df.language:\n     \
        \   metrics.log_metric(language, df[df.language == language].percentage.values[0])\n\
        \n"
      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - identify_languages
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, identify-languages, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.8'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {}, "inputArtifacts":
          {"unique_onions": {"metadataPath": "/tmp/inputs/unique_onions/data", "schemaTitle":
          "system.Artifact", "instanceSchema": "", "schemaVersion": "0.0.1"}}, "outputParameters":
          {}, "outputArtifacts": {"language_output": {"schemaTitle": "system.Artifact",
          "instanceSchema": "", "schemaVersion": "0.0.1", "metadataPath": "/tmp/outputs/language_output/data"},
          "metrics": {"schemaTitle": "system.Metrics", "instanceSchema": "", "schemaVersion":
          "0.0.1", "metadataPath": "/tmp/outputs/metrics/data"}, "plot": {"schemaTitle":
          "system.HTML", "instanceSchema": "", "schemaVersion": "0.0.1", "metadataPath":
          "/tmp/outputs/plot/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.8
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: pipeline-name}
      - {name: pipeline-root}
      artifacts:
      - {name: deduplicate-onions-nearduplicated_onions, path: /tmp/inputs/unique_onions/data}
    outputs:
      artifacts:
      - {name: identify-languages-language_output, path: /tmp/outputs/language_output/data}
      - {name: identify-languages-metrics, path: /tmp/outputs/metrics/data}
      - {name: identify-languages-plot, path: /tmp/outputs/plot/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: onion-analysis-pipeline
    inputs:
      parameters:
      - {name: from_day}
      - {name: from_month}
      - {name: from_year}
      - {name: pipeline-name}
      - {name: pipeline-root}
    dag:
      tasks:
      - name: classify-onions
        template: classify-onions
        dependencies: [identify-languages]
        arguments:
          parameters:
          - {name: from_day, value: '{{inputs.parameters.from_day}}'}
          - {name: from_month, value: '{{inputs.parameters.from_month}}'}
          - {name: from_year, value: '{{inputs.parameters.from_year}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          artifacts:
          - {name: identify-languages-language_output, from: '{{tasks.identify-languages.outputs.artifacts.identify-languages-language_output}}'}
      - name: deduplicate-onions
        template: deduplicate-onions
        dependencies: [preprocess-onions]
        arguments:
          parameters:
          - {name: from_day, value: '{{inputs.parameters.from_day}}'}
          - {name: from_month, value: '{{inputs.parameters.from_month}}'}
          - {name: from_year, value: '{{inputs.parameters.from_year}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          artifacts:
          - {name: preprocess-onions-preprocessed_onions, from: '{{tasks.preprocess-onions.outputs.artifacts.preprocess-onions-preprocessed_onions}}'}
      - name: download-onions
        template: download-onions
        arguments:
          parameters:
          - {name: from_day, value: '{{inputs.parameters.from_day}}'}
          - {name: from_month, value: '{{inputs.parameters.from_month}}'}
          - {name: from_year, value: '{{inputs.parameters.from_year}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
      - name: identify-languages
        template: identify-languages
        dependencies: [deduplicate-onions]
        arguments:
          parameters:
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          artifacts:
          - {name: deduplicate-onions-nearduplicated_onions, from: '{{tasks.deduplicate-onions.outputs.artifacts.deduplicate-onions-nearduplicated_onions}}'}
      - name: preprocess-onions
        template: preprocess-onions
        dependencies: [download-onions]
        arguments:
          parameters:
          - {name: from_day, value: '{{inputs.parameters.from_day}}'}
          - {name: from_month, value: '{{inputs.parameters.from_month}}'}
          - {name: from_year, value: '{{inputs.parameters.from_year}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          artifacts:
          - {name: download-onions-cleaned_onions, from: '{{tasks.download-onions.outputs.artifacts.download-onions-cleaned_onions}}'}
      - name: version-deduplicated-dataset
        template: version-deduplicated-dataset
        dependencies: [deduplicate-onions]
        arguments:
          parameters:
          - {name: from_day, value: '{{inputs.parameters.from_day}}'}
          - {name: from_month, value: '{{inputs.parameters.from_month}}'}
          - {name: from_year, value: '{{inputs.parameters.from_year}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          artifacts:
          - {name: deduplicate-onions-exactduplicated_onions, from: '{{tasks.deduplicate-onions.outputs.artifacts.deduplicate-onions-exactduplicated_onions}}'}
          - {name: deduplicate-onions-nearduplicated_onions, from: '{{tasks.deduplicate-onions.outputs.artifacts.deduplicate-onions-nearduplicated_onions}}'}
      - name: version-language-dataset
        template: version-language-dataset
        dependencies: [identify-languages]
        arguments:
          parameters:
          - {name: from_day, value: '{{inputs.parameters.from_day}}'}
          - {name: from_month, value: '{{inputs.parameters.from_month}}'}
          - {name: from_year, value: '{{inputs.parameters.from_year}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          artifacts:
          - {name: identify-languages-language_output, from: '{{tasks.identify-languages.outputs.artifacts.identify-languages-language_output}}'}
      - name: version-onion-dataset
        template: version-onion-dataset
        dependencies: [download-onions]
        arguments:
          parameters:
          - {name: from_day, value: '{{inputs.parameters.from_day}}'}
          - {name: from_month, value: '{{inputs.parameters.from_month}}'}
          - {name: from_year, value: '{{inputs.parameters.from_year}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          artifacts:
          - {name: download-onions-cleaned_onions, from: '{{tasks.download-onions.outputs.artifacts.download-onions-cleaned_onions}}'}
      - name: version-preprocessed-dataset
        template: version-preprocessed-dataset
        dependencies: [preprocess-onions]
        arguments:
          parameters:
          - {name: from_day, value: '{{inputs.parameters.from_day}}'}
          - {name: from_month, value: '{{inputs.parameters.from_month}}'}
          - {name: from_year, value: '{{inputs.parameters.from_year}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
          artifacts:
          - {name: preprocess-onions-preprocessed_onions, from: '{{tasks.preprocess-onions.outputs.artifacts.preprocess-onions-preprocessed_onions}}'}
  - name: preprocess-onions
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'nltk' 'plotly' 'kfp==1.8.18' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def preprocess_onions(from_day:int, from_month:int, from_year:int, cleaned_onions: Input[Artifact], preprocessed_onions: Output[Artifact], metrics: Output[Metrics], plot: Output[HTML]):
            import pickle
            import re
            import string
            from datetime import date, timedelta
            import nltk
            from nltk.tokenize import sent_tokenize, word_tokenize
            import plotly.graph_objs as go
            import plotly.io as pio
            nltk.download('punkt')

            def find_onion_address(string):
                # Use a regular expression to match a TOR onion service address
                pattern = r'\b[a-z2-7]{16,56}\.onion\b'
                match = re.search(pattern, string)
                return match is not None

            def find_pgp_key(sentence):
                # Use a regular expression to search for an email address in the input string
                regex = re.compile(r"-----BEGIN PGP PUBLIC KEY BLOCK-----.*?-----END PGP PUBLIC KEY BLOCK-----?")
                # Use the re.search() method to find a match for the regular expression
                match = re.search(regex, sentence)
                # Return a boolean value indicating whether a match was found
                return bool(match)

            def find_email_address(sentence):
                # Use a regular expression to search for an email address in the input string
                regex = re.compile(r"\s[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\s")
                # Use the re.search() method to find a match for the regular expression
                match = re.search(regex, sentence)
                # Return a boolean value indicating whether a match was found
                return bool(match)

            def find_monetary_value(sentence):
                # Use a regular expression to search for a monetary value in the input string
                regex = re.compile(r"(%|USD|EUR|\$)\s?\d+(?:[.,]\d{3})*|\d+(?:[.,]\d)*\s?(USD|EUR|\$|BTC|%)")
                # Use the re.search() method to find a match for the regular expression
                match = re.search(regex, sentence)
                # Return a boolean value indicating whether a match was found
                return bool(match)

            def find_url(sentence):
                # Use a regular expression to search for a URL in the input string
                # regex = re.compile(r"[(http(s)?):\/\/(www\.)?a-zA-Z0-9@:%._\+~#=]{2,256}\.[a-z]{2,6}\b([-a-zA-Z0-9@:%_\+.~#?&//=]*)")
                regex = re.compile(
                    r'^(?:http|ftp)s?://' # http:// or https:// or ftp:// or ftps://
                    r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+(?:[A-Z]{2,6}\.?|[A-Z0-9-]{2,}\.?)|' # domain...
                    r'(localhost)|' # localhost...
                    r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})' # ...or ip
                    r'(?::\d+)?' # optional port
                    r'(?:/?|[/?]\S+)$', re.IGNORECASE)
                # Use the re.search() method to find a match for the regular expression
                match = re.search(regex, sentence)
                # Return a boolean value indicating whether a match was found
                return bool(match)

            def find_bitcoin_address(sentence):
                # Use a regular expression to search for a Bitcoin address in the input string
                regex = re.compile(r"([13]|bc1)[A-HJ-NP-Za-km-z1-9]{27,34}")
                # Use the re.search() method to find a match for the regular expression
                match = re.search(regex, sentence)
                # Return a boolean value indicating whether a match was found
                return bool(match)

            def is_web_tag(sentence):
                tags = ["Contact", "Logic", "FAQ", "Login", "About", "Services", "Privacy", "Term", "Support", "Blog", "Team", "Contact Us", "Sign Up", "Sign In", "Login", "Register", "Dark Web", "TOR", "Onion Router", "Hidden Services", "Encryption", "Anonymity"]
                # Use a regular expression to search for words in the given sentence in the array of tags
                return sentence in tags

            def check_for_words(text):
                # Use a regular expression to match any word that contains at least one alphabetical character
                pattern = r'\b[a-zA-Z]+\b'
                match = re.search(pattern, text)
                return match is not None

            def remove_line_breaks_and_tabs(text):
                # Replace line breaks with spaces
                text = text.replace('\n', '. ')

                # Replace tabs with spaces
                text = text.replace('\t', '. ')

                return text

            def remove_special_characters(text):
                # Use the string.translate() method to remove specific characters from the input string
                text = text.translate(str.maketrans('', '', '<#*_="+><'))
                return text

            def remove_non_word_characters_at_beginning(s):
                # Make a translation table to remove non-word characters
                table = s.maketrans('', '', string.punctuation + string.whitespace + '0123456789')

                # Split the string into a list of words
                words = s.split()

                # Find the first word in the list
                for i, word in enumerate(words):
                    # Use the translation table to remove non-word characters from the beginning of the word
                    stripped = word.lstrip(string.punctuation + string.whitespace + '0123456789')
                    # If the stripped word is not empty, it is the first word
                    if stripped:
                        words[i] = stripped
                        break
                    # Otherwise, remove the word from the list
                    else:
                        words.pop(i)

                # Join the words back into a single string, with a single space between each word
                return ' '.join(words)

            def remove_spaces_before_punctuation(string):
                # Split the string into a list of characters
                chars = list(string)

                # Initialize an empty list to store the modified characters
                fixed_chars = []

                # Iterate through the characters
                for i, char in enumerate(chars):
                    # If the character is a punctuation mark and the previous character is a space,
                    # skip the space and add the punctuation mark to the list of modified characters
                    if char in ['!', '?', ',', ';', ':'] and i > 0 and chars[i-1] == ' ':
                        fixed_chars[-1] = char
                    # If the character is not a punctuation mark, or if it is a punctuation mark but the previous character is not a space,
                    # add the character to the list of modified characters
                    else:
                        fixed_chars.append(char)

                # Join the modified characters into a single string and return it
                return ''.join(fixed_chars)

            def deduplicate_words(sentence):
                # Split the sentence into a list of words
                words = sentence.split(" ")

                # Create an empty list to store the deduplicated words
                deduplicated = [words[0]]

                # Loop through each word in the list of words
                for word in words[1:]:
                    # If the current word is not the same as the previous word, add it to the list of deduplicated words
                    if word != deduplicated[-1]:
                        deduplicated.append(word)
                # Join the deduplicated words into a string and return it
                return " ".join(deduplicated)

            def compact_repeated_spaces(input_string):
                # Use the re.sub() method to remove repeated spaces from the input string
                output_string = re.sub(r'\s+', ' ', input_string)
                return output_string

            def modify_string(input_string):
                # Use the string.replace() method to replace certain characters or sequences of characters in the input string
                input_string = input_string.replace('#','.')
                input_string = input_string.replace(' -','. ')
                input_string = input_string.replace(':)','')
                input_string = input_string.replace(' _','. ')
                input_string = input_string.replace(' |','. ')
                input_string = input_string.replace(' !','. ')
                input_string = input_string.replace('!','. ')
                input_string = input_string.replace('?','? ')
                input_string = input_string.replace(' ?','? ')
                input_string = input_string.replace(' :',':')
                input_string = input_string.replace('( ',' (')
                input_string = input_string.replace(' )',') ')
                input_string = input_string.replace(' ;','. ')
                input_string = input_string.replace(';','. ')
                input_string = input_string.replace(' .','. ')
                input_string = input_string.replace(':.',':')
                input_string = input_string.replace('?.','?')
                input_string = input_string.replace('-.','.')
                input_string = compact_repeated_spaces(input_string)
                return input_string.strip()

            def remove_repeating_punctuation(string):
                # Use a regular expression to match one or more repetitions of any punctuation character
                pattern = r'([^\w\s])\1+'
                # Replace all repetitions with a single instance of the punctuation character
                return re.sub(pattern, r'\1', string)

            def deduplicate_sentences(text):
                # Split the text into a list of sentences
                sentences = text.split(".")

                # Create an empty list to store the deduplicated sentences
                deduplicated = [sentences[0]]

                # Loop through each sentence in the list of sentences
                for sentence in sentences[1:]:
                    # If the current sentence is not the same as the previous sentence, add it to the list of deduplicated sentences
                    if sentence != deduplicated[-1]:
                        deduplicated.append(sentence)

                # Join the deduplicated sentences into a string and return it
                return ".".join(deduplicated)

            def remove_noise(input_string, debug=False):

                onion = ''
                for sentence in sent_tokenize(input_string):

                    if debug:
                        print(sentence)
                        print("------------------------------")

                    # discard emails, prices, urls, bitcoin addresses
                    if not find_onion_address(sentence) and not find_pgp_key(sentence) and not find_email_address(sentence) and not find_monetary_value(sentence) and not find_url(sentence) and not find_bitcoin_address(sentence) and not is_web_tag(sentence) and check_for_words:  # last preventive check for particular web-based keywords

                        if debug:
                            print(sentence)
                            print("*********")

                        sentence = remove_line_breaks_and_tabs(sentence)
                        sentence = remove_special_characters(sentence)
                        sentence = re.sub(' +', ' ', sentence)  # Replace multiple spaces with a single space
                        sentence = remove_non_word_characters_at_beginning(sentence)
                        sentence = remove_repeating_punctuation(sentence)
                        sentence = remove_spaces_before_punctuation(sentence)
                        sentence = deduplicate_words(sentence)

                        if len(sentence) > 0 and sentence[-1] not in string.punctuation:
                            sentence = sentence+'.'

                        sentence = sentence.strip()

                        # Capitalize the first letter of each sentence
                        sentence = re.sub(r'(^|[.!?])\s*([a-z])', lambda x: x.group(1) + ' ' + x.group(2).upper(), sentence)
                        onion = "{0}{1} ".format(onion, sentence)

                    else:
                        if debug:
                            print("XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX")
                            print(sentence)
                            print(f"Words: {not check_for_words(sentence)}")
                            print(f"Onion: {find_onion_address(sentence)}")
                            print(f"PGP: {find_pgp_key(sentence)}")
                            print(f"Email: {find_email_address(sentence)}")
                            print(f"Monetary: {find_monetary_value(sentence)}")
                            print(f"URL: {find_url(sentence)}")
                            print(f"Bitcoin: {find_bitcoin_address(sentence)}")
                            print(f"Webtag: {is_web_tag(sentence)}")
                            print("XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX")


                onion = modify_string(onion)
                onion = remove_repeating_punctuation(onion)
                onion = deduplicate_sentences(onion)
                return onion.strip()

            ponions = {}
            nempty_onions = 0
            ncleaned_onions = 0

            with open(cleaned_onions.path, 'rb') as f:
                onions = pickle.load(f)

            for address, text in onions.items():
                if text not in ['', 'bad encoded']:
                    ncleaned_onions+=1
                    cleaned_text = remove_noise(text)
                    if cleaned_text == '':
                        nempty_onions += 1
                    ponions[address] = cleaned_text

                #if ncleaned_onions % 10 == 0:
                #    print(f"Preprocessed {ncleaned_onions}...")


            yesterday = date.today()- timedelta(days=1)
            fig = go.Figure(data=[go.Pie(labels=['Preprocessed onions', 'Empty onions (after preprocessing)'],
                                         values=[ncleaned_onions-nempty_onions, nempty_onions])])
            fig.update_layout(title=f'Preprocessed onions from {from_day}/{from_month}/{from_year} to {yesterday.day}/{yesterday.month}/{yesterday.year}')
            with open(plot.path, 'w') as p:
                p.write(pio.to_html(fig))

            metrics.log_metric('Cleaned onions', ncleaned_onions)
            metrics.log_metric('Empty onions (after preprocessing)', nempty_onions)
            metrics.log_metric('Preprocessed onions', ncleaned_onions-nempty_onions)
            with open(preprocessed_onions.path, 'wb') as f:
                pickle.dump(ponions, f)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - preprocess_onions
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, preprocess-onions, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'from_day={{inputs.parameters.from_day}}',
        'from_month={{inputs.parameters.from_month}}', 'from_year={{inputs.parameters.from_year}}',
        --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.9-alpine'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"from_day": {"type":
          "INT"}, "from_month": {"type": "INT"}, "from_year": {"type": "INT"}}, "inputArtifacts":
          {"cleaned_onions": {"metadataPath": "/tmp/inputs/cleaned_onions/data", "schemaTitle":
          "system.Artifact", "instanceSchema": "", "schemaVersion": "0.0.1"}}, "outputParameters":
          {}, "outputArtifacts": {"metrics": {"schemaTitle": "system.Metrics", "instanceSchema":
          "", "schemaVersion": "0.0.1", "metadataPath": "/tmp/outputs/metrics/data"},
          "plot": {"schemaTitle": "system.HTML", "instanceSchema": "", "schemaVersion":
          "0.0.1", "metadataPath": "/tmp/outputs/plot/data"}, "preprocessed_onions":
          {"schemaTitle": "system.Artifact", "instanceSchema": "", "schemaVersion":
          "0.0.1", "metadataPath": "/tmp/outputs/preprocessed_onions/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.9-alpine
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: from_day}
      - {name: from_month}
      - {name: from_year}
      - {name: pipeline-name}
      - {name: pipeline-root}
      artifacts:
      - {name: download-onions-cleaned_onions, path: /tmp/inputs/cleaned_onions/data}
    outputs:
      artifacts:
      - {name: preprocess-onions-metrics, path: /tmp/outputs/metrics/data}
      - {name: preprocess-onions-plot, path: /tmp/outputs/plot/data}
      - {name: preprocess-onions-preprocessed_onions, path: /tmp/outputs/preprocessed_onions/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
        pipelines.kubeflow.org/arguments.parameters: '{"from_day": "{{inputs.parameters.from_day}}",
          "from_month": "{{inputs.parameters.from_month}}", "from_year": "{{inputs.parameters.from_year}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: version-deduplicated-dataset
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'minio' 'plotly' 'kfp==1.8.18' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        import kfp
        from kfp.v2 import dsl
        from kfp.v2.dsl import *
        from typing import *

        def version_deduplicated_dataset(from_day:int, from_month:int, from_year:int, exactduplicated_onions: Input[Artifact], nearduplicated_onions: Input[Artifact], metrics: Output[Metrics], plot: Output[HTML]):
            from minio import Minio
            import pickle
            from datetime import date, timedelta
            from minio.error import S3Error
            import io
            import plotly.graph_objs as go
            import plotly.io as pio

            client = Minio(
                endpoint="minio.minio:9000",
                secure=False,
                access_key='processing',
                secret_key=',TkTV3c7:e6#e}HwiD4R'
            )

            with open(exactduplicated_onions.path, 'rb') as f:
                exact_duplicated_onions = pickle.load(f)

            with open(nearduplicated_onions.path, 'rb') as f:
                near_duplicated_onions = pickle.load(f)


            ##### write to MinIO
            versioning_date = date.today() - timedelta(days=1)

            bytes_file = pickle.dumps(exact_duplicated_onions)
            client.put_object(
                    bucket_name="datasets",
                    object_name=f"{versioning_date.year}/{versioning_date.month}/{versioning_date.day}/deduplicated_onions/exact_onions.pickle",
                    data=io.BytesIO(bytes_file),
                    length=len(bytes_file))

            bytes_file = pickle.dumps(near_duplicated_onions)
            client.put_object(
                    bucket_name="datasets",
                    object_name=f"{versioning_date.year}/{versioning_date.month}/{versioning_date.day}/deduplicated_onions/near_onions.pickle",
                    data=io.BytesIO(bytes_file),
                    length=len(bytes_file))

            bytes_file = pickle.dumps(list(near_duplicated_onions.keys()))
            client.put_object(
                    bucket_name="datasets",
                    object_name=f"{versioning_date.year}/{versioning_date.month}/{versioning_date.day}/deduplicated_onions/unique_onions.pickle",
                    data=io.BytesIO(bytes_file),
                    length=len(bytes_file))


            ### stats

            total_onions = 0
            exact_duplicates = 0
            instances = {}
            for text, addresses in exact_duplicated_onions.items():
                exact_duplicates += len(addresses)-1
                total_onions += len(addresses)-1

            near_duplicates = 0
            for text, near_texts in near_duplicated_onions.items():
                near_duplicates += len(near_texts)-1
                total_onions += len(near_texts)
                instances[text] = len(near_texts)
                instances[text] += len(exact_duplicated_onions[text])-1

            final_unique_onions = len(near_duplicated_onions.keys())

            stats = f"Preprocessed onions: {total_onions}\nExact mirrors: {exact_duplicates}\nNear mirrors: {near_duplicates}\nUnique onions: {final_unique_onions}"
            stats_bytes = stats.encode('utf-8')
            client.put_object(
                    bucket_name="datasets",
                    object_name=f"{versioning_date.year}/{versioning_date.month}/{versioning_date.day}/deduplicated_onions/stats.txt",
                    data=io.BytesIO(stats_bytes),
                    length=len(stats_bytes))


            #### metrics and plot
            fig = go.Figure(data=[go.Pie(labels=['Total exact mirrors', 'Total near mirrors', 'Total unique onions'],
                                         values=[exact_duplicates, near_duplicates, final_unique_onions])])
            fig.update_layout(title=f'Total duplicated onions until {versioning_date.day}/{versioning_date.month}/{versioning_date.year}')

            #### save the boxplot distribution of instances in html boxplot variable
            fig2 = go.Figure(data=[go.Box(y=list(instances.values()), boxpoints=False)])
            fig2.update_layout(title=f'Boxplot of instances of each onion until {versioning_date.day}/{versioning_date.month}/{versioning_date.year}')

            with open(plot.path, 'w') as p:
                p.write(pio.to_html(fig)+pio.to_html(fig2))

            metrics.log_metric('Total onions', total_onions)
            metrics.log_metric('Total exact mirrors', exact_duplicates)
            metrics.log_metric('Total near mirrors', near_duplicates)
            metrics.log_metric('Total unique onions', final_unique_onions)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - version_deduplicated_dataset
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, version-deduplicated-dataset,
        --pipeline_name, '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID),
        --run_resource, workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE),
        --pod_name, $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'from_day={{inputs.parameters.from_day}}',
        'from_month={{inputs.parameters.from_month}}', 'from_year={{inputs.parameters.from_year}}',
        --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.8-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"from_day": {"type":
          "INT"}, "from_month": {"type": "INT"}, "from_year": {"type": "INT"}}, "inputArtifacts":
          {"exactduplicated_onions": {"metadataPath": "/tmp/inputs/exactduplicated_onions/data",
          "schemaTitle": "system.Artifact", "instanceSchema": "", "schemaVersion":
          "0.0.1"}, "nearduplicated_onions": {"metadataPath": "/tmp/inputs/nearduplicated_onions/data",
          "schemaTitle": "system.Artifact", "instanceSchema": "", "schemaVersion":
          "0.0.1"}}, "outputParameters": {}, "outputArtifacts": {"metrics": {"schemaTitle":
          "system.Metrics", "instanceSchema": "", "schemaVersion": "0.0.1", "metadataPath":
          "/tmp/outputs/metrics/data"}, "plot": {"schemaTitle": "system.HTML", "instanceSchema":
          "", "schemaVersion": "0.0.1", "metadataPath": "/tmp/outputs/plot/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.8-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: from_day}
      - {name: from_month}
      - {name: from_year}
      - {name: pipeline-name}
      - {name: pipeline-root}
      artifacts:
      - {name: deduplicate-onions-exactduplicated_onions, path: /tmp/inputs/exactduplicated_onions/data}
      - {name: deduplicate-onions-nearduplicated_onions, path: /tmp/inputs/nearduplicated_onions/data}
    outputs:
      artifacts:
      - {name: version-deduplicated-dataset-metrics, path: /tmp/outputs/metrics/data}
      - {name: version-deduplicated-dataset-plot, path: /tmp/outputs/plot/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
        pipelines.kubeflow.org/arguments.parameters: '{"from_day": "{{inputs.parameters.from_day}}",
          "from_month": "{{inputs.parameters.from_month}}", "from_year": "{{inputs.parameters.from_year}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: version-language-dataset
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'minio' 'plotly' 'pandas' 'kfp==1.8.18' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
        \ import *\n\ndef version_language_dataset(from_day:int, from_month:int, from_year:int,\
        \ language_input: Input[Artifact], plot: Output[HTML], metrics: Output[Metrics]):\n\
        \n    # get from MinIO the onion_languages dictionary of previous version\n\
        \    from minio import Minio\n    from minio import S3Error\n    import pickle\n\
        \    from datetime import date, timedelta\n\n    client = Minio(\n       \
        \ endpoint=\"minio.minio:9000\",\n        secure=False,\n        access_key='processing',\n\
        \        secret_key=',TkTV3c7:e6#e}HwiD4R')\n\n\n    with open(language_input.path,\
        \ 'rb') as f:\n        onion_languages = pickle.load(f)\n\n    # download\
        \ last version\n    first_version = date(2022, 11, 15)                   \
        \   # threshold of first version\n    last_date = date(from_year, from_month,\
        \ from_day)        # dates to check from now to origin\n    last_dataset =\
        \ None                                     # indicates if previous version\
        \ found\n\n    while (last_date > first_version and last_dataset is None):\n\
        \        last_date = last_date - timedelta(days=1)\n        print(\"Checking\
        \ version of\", last_date.strftime(\"%d-%m-%Y\"), \"...\")\n        try:\n\
        \            response = client.get_object(\"datasets\",f\"{last_date.year}/{last_date.month}/{last_date.day}/languages/languages.pickle\"\
        )\n            last_dataset = pickle.loads(response.read())\n        except\
        \ S3Error as exc:\n            print(\"It does not exist! Next...\", exc)\n\
        \            continue\n\n    # update to new version \n    if last_dataset\
        \ is None:\n        updated_dataset = onion_languages\n    else:\n       \
        \ last_dataset.update(onion_languages)\n        updated_dataset = last_dataset\n\
        \n    ##### write to MinIO\n    import io\n\n    versioning_date = date.today()\
        \ - timedelta(days=1)\n    bytes_file = pickle.dumps(updated_dataset)\n  \
        \  client.put_object(\n            bucket_name=\"datasets\",\n           \
        \ object_name=f\"{versioning_date.year}/{versioning_date.month}/{versioning_date.day}/languages/languages.pickle\"\
        ,\n            data=io.BytesIO(bytes_file),\n            length=len(bytes_file))\n\
        \n    # plot the distribution of languages of the first language in onion_languages[onion]['first_language']\n\
        \    import pandas as pd\n    import plotly.graph_objects as go\n    import\
        \ plotly.io as pio\n\n    df = pd.DataFrame(updated_dataset).T\n    df = df.first_language.value_counts()/len(df)*100\n\
        \    df = df.reset_index()\n    df.columns = ['language', 'percentage']\n\
        \    df = df.sort_values(by='percentage', ascending=False)\n    df = df.head(10)\n\
        \    fig = go.Figure(data=[go.Pie(labels=df.language, values=df.percentage)])\n\
        \    fig.update_layout(title=f'Language distribution of onions until {versioning_date.day}/{versioning_date.month}/{versioning_date.year}')\n\
        \    with open(plot.path, 'w') as p:\n        p.write(pio.to_html(fig))\n\n\
        \n    ## write to MinIO the stats of the first 10 languages in text file\n\
        \    import io\n    stats = df.to_string(index=False)\n    client.put_object(\n\
        \            bucket_name=\"datasets\",\n            object_name=f\"{versioning_date.year}/{versioning_date.month}/{versioning_date.day}/languages/stats.txt\"\
        ,\n            data=io.BytesIO(stats.encode()),\n            length=len(stats))\n\
        \n\n    # write in metrics the number of onions per language\n    metrics.log_metric('Total\
        \ onions', len(updated_dataset))\n    for language in df.language:\n     \
        \   metrics.log_metric(language, df[df.language == language].percentage.values[0])\n\
        \n"
      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - version_language_dataset
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, version-language-dataset,
        --pipeline_name, '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID),
        --run_resource, workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE),
        --pod_name, $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'from_day={{inputs.parameters.from_day}}',
        'from_month={{inputs.parameters.from_month}}', 'from_year={{inputs.parameters.from_year}}',
        --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.8-slim'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"from_day": {"type":
          "INT"}, "from_month": {"type": "INT"}, "from_year": {"type": "INT"}}, "inputArtifacts":
          {"language_input": {"metadataPath": "/tmp/inputs/language_input/data", "schemaTitle":
          "system.Artifact", "instanceSchema": "", "schemaVersion": "0.0.1"}}, "outputParameters":
          {}, "outputArtifacts": {"metrics": {"schemaTitle": "system.Metrics", "instanceSchema":
          "", "schemaVersion": "0.0.1", "metadataPath": "/tmp/outputs/metrics/data"},
          "plot": {"schemaTitle": "system.HTML", "instanceSchema": "", "schemaVersion":
          "0.0.1", "metadataPath": "/tmp/outputs/plot/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.8-slim
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: from_day}
      - {name: from_month}
      - {name: from_year}
      - {name: pipeline-name}
      - {name: pipeline-root}
      artifacts:
      - {name: identify-languages-language_output, path: /tmp/inputs/language_input/data}
    outputs:
      artifacts:
      - {name: version-language-dataset-metrics, path: /tmp/outputs/metrics/data}
      - {name: version-language-dataset-plot, path: /tmp/outputs/plot/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
        pipelines.kubeflow.org/arguments.parameters: '{"from_day": "{{inputs.parameters.from_day}}",
          "from_month": "{{inputs.parameters.from_month}}", "from_year": "{{inputs.parameters.from_year}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: version-onion-dataset
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'minio' 'plotly' 'kfp==1.8.18' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
        \ import *\n\ndef version_onion_dataset(from_day:int, from_month:int, from_year:int,\
        \ cleaned_onions: Input[Artifact], metrics: Output[Metrics], plot: Output[HTML]):\n\
        \    from minio import Minio\n    import pickle\n    from datetime import\
        \ date, timedelta\n    from minio.error import S3Error\n    import io\n  \
        \  import plotly.graph_objs as go\n    import plotly.io as pio\n\n    client\
        \ = Minio(\n        endpoint=\"minio.minio:9000\",\n        secure=False,\n\
        \        access_key='processing',\n        secret_key=',TkTV3c7:e6#e}HwiD4R'\n\
        \    )\n\n    with open(cleaned_onions.path, 'rb') as f:\n        new_onions\
        \ = pickle.load(f)\n\n\n    # download previous version\n    first_version\
        \ = date(2022, 11, 15)                          # threshold of first version\n\
        \    last_date = date(from_year, from_month, from_day)           # dates to\
        \ check from now to origin\n    last_dataset = None                      \
        \                   # indicates if previous version found\n\n    while (last_date\
        \ > first_version and last_dataset is None):\n        last_date = last_date\
        \ - timedelta(days=1)\n        print(\"Checking version of\", last_date.strftime(\"\
        %d-%m-%Y\"), \"...\")\n        try:\n            response = client.get_object(\"\
        datasets\",f\"{last_date.year}/{last_date.month}/{last_date.day}/cleaned_onions/cleaned_onions.pickle\"\
        )\n            print(\"Downloaded version of\", last_date.strftime(\"%d-%m-%Y\"\
        ))\n            last_dataset = pickle.loads(response.read())\n        except\
        \ S3Error as exc:\n            print(\"It does not exist! Next...\", exc)\n\
        \            continue\n\n    # update to new version \n    if last_dataset\
        \ is None:\n        updated_dataset = new_onions\n    else:\n        last_dataset.update(new_onions)\n\
        \        updated_dataset = last_dataset\n\n    total_collected_onions = 0\n\
        \    total_empty_onions = 0\n    total_bad_encoded_onions = 0\n    for onion,\
        \ text in updated_dataset.items():\n        total_collected_onions +=1\n \
        \       if text == '':\n            total_empty_onions+=1\n        elif text\
        \ == 'bad encoded':\n            total_bad_encoded_onions+=1\n\n\n    #####\
        \ write to MinIO\n    versioning_date = date.today() - timedelta(days=1) \
        \   # date for new versioning\n\n    bytes_file = pickle.dumps(updated_dataset)\n\
        \    client.put_object(\n            bucket_name=\"datasets\",\n         \
        \   object_name=f\"{versioning_date.year}/{versioning_date.month}/{versioning_date.day}/cleaned_onions/cleaned_onions.pickle\"\
        ,\n            data=io.BytesIO(bytes_file),\n            length=len(bytes_file))\n\
        \n    stats = f\"Total collected onions: {total_collected_onions}\\nTotal\
        \ empty onions: {total_empty_onions}\\nTotal bad encoded: {total_bad_encoded_onions}\\\
        nTotal cleaned onions: {total_collected_onions-total_empty_onions-total_bad_encoded_onions}\"\
        \n    stats_bytes = stats.encode('utf-8')\n    client.put_object(\n      \
        \      bucket_name=\"datasets\",\n            object_name=f\"{versioning_date.year}/{versioning_date.month}/{versioning_date.day}/cleaned_onions/stats.txt\"\
        ,\n            data=io.BytesIO(stats_bytes),\n            length=len(stats_bytes))\n\
        \n    #### metrics and plot\n    fig = go.Figure(data=[go.Pie(labels=['Total\
        \ empty onions', 'Total bad encoded', 'Total cleaned onions'],\n         \
        \                        values=[total_empty_onions, total_bad_encoded_onions,\
        \ total_collected_onions-total_empty_onions-total_bad_encoded_onions])])\n\
        \    fig.update_layout(title=f'Total onions until {versioning_date.day}/{versioning_date.month}/{versioning_date.year}')\n\
        \    with open(plot.path, 'w') as p:\n        p.write(pio.to_html(fig))\n\n\
        \    metrics.log_metric('Total collected onions', total_collected_onions)\n\
        \    metrics.log_metric('Total empty onions', total_empty_onions)\n    metrics.log_metric('Total\
        \ bad encoded', total_bad_encoded_onions)\n    metrics.log_metric('Total cleaned\
        \ onions', total_collected_onions-total_empty_onions-total_bad_encoded_onions)\n\
        \n"
      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - version_onion_dataset
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, version-onion-dataset, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'from_day={{inputs.parameters.from_day}}',
        'from_month={{inputs.parameters.from_month}}', 'from_year={{inputs.parameters.from_year}}',
        --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.9-alpine'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"from_day": {"type":
          "INT"}, "from_month": {"type": "INT"}, "from_year": {"type": "INT"}}, "inputArtifacts":
          {"cleaned_onions": {"metadataPath": "/tmp/inputs/cleaned_onions/data", "schemaTitle":
          "system.Artifact", "instanceSchema": "", "schemaVersion": "0.0.1"}}, "outputParameters":
          {}, "outputArtifacts": {"metrics": {"schemaTitle": "system.Metrics", "instanceSchema":
          "", "schemaVersion": "0.0.1", "metadataPath": "/tmp/outputs/metrics/data"},
          "plot": {"schemaTitle": "system.HTML", "instanceSchema": "", "schemaVersion":
          "0.0.1", "metadataPath": "/tmp/outputs/plot/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.9-alpine
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: from_day}
      - {name: from_month}
      - {name: from_year}
      - {name: pipeline-name}
      - {name: pipeline-root}
      artifacts:
      - {name: download-onions-cleaned_onions, path: /tmp/inputs/cleaned_onions/data}
    outputs:
      artifacts:
      - {name: version-onion-dataset-metrics, path: /tmp/outputs/metrics/data}
      - {name: version-onion-dataset-plot, path: /tmp/outputs/plot/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
        pipelines.kubeflow.org/arguments.parameters: '{"from_day": "{{inputs.parameters.from_day}}",
          "from_month": "{{inputs.parameters.from_month}}", "from_year": "{{inputs.parameters.from_year}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: version-preprocessed-dataset
    container:
      args:
      - sh
      - -c
      - |2

        if ! [ -x "$(command -v pip)" ]; then
            python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
        fi

        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'minio' 'plotly' 'kfp==1.8.18' && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
        \ import *\n\ndef version_preprocessed_dataset(from_day:int, from_month:int,\
        \ from_year:int, preprocessed_onions: Input[Artifact], metrics: Output[Metrics],\
        \ plot: Output[HTML]):\n    from minio import Minio\n    import pickle\n \
        \   from datetime import date, timedelta\n    from minio.error import S3Error\n\
        \    import io\n    import plotly.graph_objs as go\n    import plotly.io as\
        \ pio\n\n    client = Minio(\n        endpoint=\"minio.minio:9000\",\n   \
        \     secure=False,\n        access_key='processing',\n        secret_key=',TkTV3c7:e6#e}HwiD4R'\n\
        \    )\n\n    with open(preprocessed_onions.path, 'rb') as f:\n        new_onions\
        \ = pickle.load(f)\n\n\n    # download last version\n    first_version = date(2022,\
        \ 11, 15)                      # threshold of first version\n    last_date\
        \ = date(from_year, from_month, from_day)        # dates to check from now\
        \ to origin\n    last_dataset = None                                     #\
        \ indicates if previous version found\n\n    while (last_date > first_version\
        \ and last_dataset is None):\n        last_date = last_date - timedelta(days=1)\n\
        \        print(\"Checking version of\", last_date.strftime(\"%d-%m-%Y\"),\
        \ \"...\")\n        try:\n            response = client.get_object(\"datasets\"\
        ,f\"{last_date.year}/{last_date.month}/{last_date.day}/preprocessed_onions/preprocessed_onions.pickle\"\
        )\n            last_dataset = pickle.loads(response.read())\n        except\
        \ S3Error as exc:\n            print(\"It does not exist! Next...\", exc)\n\
        \            continue\n\n    # update to new version \n    if last_dataset\
        \ is None:\n        updated_dataset = new_onions\n    else:\n        last_dataset.update(new_onions)\n\
        \        updated_dataset = last_dataset\n\n    total_cleaned_onions = 0\n\
        \    total_empty_onions = 0\n    for onion, text in updated_dataset.items():\n\
        \        total_cleaned_onions +=1\n        if text == '':\n            total_empty_onions+=1\n\
        \n\n    ##### write to MinIO\n    versioning_date = date.today() - timedelta(days=1)\n\
        \n    bytes_file = pickle.dumps(updated_dataset)\n    client.put_object(\n\
        \            bucket_name=\"datasets\",\n            object_name=f\"{versioning_date.year}/{versioning_date.month}/{versioning_date.day}/preprocessed_onions/preprocessed_onions.pickle\"\
        ,\n            data=io.BytesIO(bytes_file),\n            length=len(bytes_file))\n\
        \n    stats = f\"Total cleaned onions: {total_cleaned_onions}\\nTotal empty\
        \ onions: {total_empty_onions}\\nTotal preprocessed onions: {total_cleaned_onions-total_empty_onions}\"\
        \n    stats_bytes = stats.encode('utf-8')\n    client.put_object(\n      \
        \      bucket_name=\"datasets\",\n            object_name=f\"{versioning_date.year}/{versioning_date.month}/{versioning_date.day}/preprocessed_onions/stats.txt\"\
        ,\n            data=io.BytesIO(stats_bytes),\n            length=len(stats_bytes))\n\
        \n    #### metrics and plot\n    fig = go.Figure(data=[go.Pie(labels=['Total\
        \ empty onions (after preprocessing)', 'Total preprocessed onions'],\n   \
        \                              values=[total_empty_onions, total_cleaned_onions-total_empty_onions])])\n\
        \    fig.update_layout(title=f'Total onions until {versioning_date.day}/{versioning_date.month}/{versioning_date.year}')\n\
        \    with open(plot.path, 'w') as p:\n        p.write(pio.to_html(fig))\n\n\
        \    metrics.log_metric('Total cleaned onions', total_cleaned_onions)\n  \
        \  metrics.log_metric('Total empty onions (after preprocessing)', total_empty_onions)\n\
        \    metrics.log_metric('Total preprocessed onions', total_cleaned_onions-total_empty_onions)\n\
        \n"
      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - version_preprocessed_dataset
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, version-preprocessed-dataset,
        --pipeline_name, '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID),
        --run_resource, workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE),
        --pod_name, $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
        --enable_caching, $(ENABLE_CACHING), --, 'from_day={{inputs.parameters.from_day}}',
        'from_month={{inputs.parameters.from_month}}', 'from_year={{inputs.parameters.from_year}}',
        --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.9-alpine'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"from_day": {"type":
          "INT"}, "from_month": {"type": "INT"}, "from_year": {"type": "INT"}}, "inputArtifacts":
          {"preprocessed_onions": {"metadataPath": "/tmp/inputs/preprocessed_onions/data",
          "schemaTitle": "system.Artifact", "instanceSchema": "", "schemaVersion":
          "0.0.1"}}, "outputParameters": {}, "outputArtifacts": {"metrics": {"schemaTitle":
          "system.Metrics", "instanceSchema": "", "schemaVersion": "0.0.1", "metadataPath":
          "/tmp/outputs/metrics/data"}, "plot": {"schemaTitle": "system.HTML", "instanceSchema":
          "", "schemaVersion": "0.0.1", "metadataPath": "/tmp/outputs/plot/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.9-alpine
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: from_day}
      - {name: from_month}
      - {name: from_year}
      - {name: pipeline-name}
      - {name: pipeline-root}
      artifacts:
      - {name: preprocess-onions-preprocessed_onions, path: /tmp/inputs/preprocessed_onions/data}
    outputs:
      artifacts:
      - {name: version-preprocessed-dataset-metrics, path: /tmp/outputs/metrics/data}
      - {name: version-preprocessed-dataset-plot, path: /tmp/outputs/plot/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
        pipelines.kubeflow.org/arguments.parameters: '{"from_day": "{{inputs.parameters.from_day}}",
          "from_month": "{{inputs.parameters.from_month}}", "from_year": "{{inputs.parameters.from_year}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.8.7
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  arguments:
    parameters:
    - {name: from_day}
    - {name: from_month}
    - {name: from_year}
    - {name: pipeline-root, value: ''}
    - {name: pipeline-name, value: pipeline/onion-analysis-pipeline}
  serviceAccountName: pipeline-runner
